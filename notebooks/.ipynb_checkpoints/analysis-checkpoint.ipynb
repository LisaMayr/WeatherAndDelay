{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9969be20",
      "metadata": {},
      "source": [
        "# Analysis Layer\n",
        "\n",
        "This notebook explores delay-weather relationships, time series patterns, duration analysis,\n",
        "a simple predictive model, and text analytics.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "063258a4",
      "metadata": {},
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "except ImportError:\n",
        "    sns = None\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 50)\n",
        "pd.set_option(\"display.width\", 120)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "36e90364",
      "metadata": {},
      "source": [
        "def _find_path(candidates):\n",
        "    for candidate in candidates:\n",
        "        path = Path(candidate)\n",
        "        if path.exists():\n",
        "            return path\n",
        "    return None\n",
        "\n",
        "CLEANED_CSV = os.getenv(\"CLEANED_CSV_PATH\", \"merged_wienerlinien_hw25.csv\")\n",
        "DAILY_CSV = os.getenv(\"DAILY_CSV_PATH\", \"wienerlinien_hw25_daily.csv\")\n",
        "\n",
        "cleaned_path = _find_path([\n",
        "    CLEANED_CSV,\n",
        "    Path(\"notebooks\") / CLEANED_CSV,\n",
        "    Path.cwd() / CLEANED_CSV,\n",
        "    Path.cwd() / \"notebooks\" / CLEANED_CSV,\n",
        "])\n",
        "daily_path = _find_path([\n",
        "    DAILY_CSV,\n",
        "    Path(\"notebooks\") / DAILY_CSV,\n",
        "    Path.cwd() / DAILY_CSV,\n",
        "    Path.cwd() / \"notebooks\" / DAILY_CSV,\n",
        "])\n",
        "\n",
        "if cleaned_path is None:\n",
        "    raise FileNotFoundError(f\"Could not find cleaned CSV: {CLEANED_CSV}\")\n",
        "if daily_path is None:\n",
        "    raise FileNotFoundError(f\"Could not find daily CSV: {DAILY_CSV}\")\n",
        "\n",
        "cleaned = pd.read_csv(cleaned_path, parse_dates=[\"start_time\", \"end_time\"])\n",
        "daily_summary = pd.read_csv(daily_path, parse_dates=[\"date\"])\n",
        "\n",
        "print(f\"Loaded cleaned: {cleaned.shape} from {cleaned_path}\")\n",
        "print(f\"Loaded daily: {daily_summary.shape} from {daily_path}\")\n",
        "cleaned.head()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "bf08e5f1",
      "metadata": {},
      "source": [
        "## Delay-Weather Relationship\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "fc0f4b63",
      "metadata": {},
      "source": [
        "target_cols = [\"RR\", \"FF\", \"TL\"]\n",
        "available_cols = [c for c in target_cols if c in daily_summary.columns]\n",
        "\n",
        "if not available_cols:\n",
        "    print(\"No RR/FF/TL columns found in daily_summary.\")\n",
        "else:\n",
        "    for col in available_cols:\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        if sns is not None:\n",
        "            sns.scatterplot(data=daily_summary, x=col, y=\"delay_count\")\n",
        "        else:\n",
        "            plt.scatter(daily_summary[col], daily_summary[\"delay_count\"], alpha=0.6)\n",
        "            plt.xlabel(col)\n",
        "            plt.ylabel(\"delay_count\")\n",
        "        plt.title(f\"Delay Count vs {col}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    corr_cols = [\"delay_count\"] + available_cols\n",
        "    print(daily_summary[corr_cols].corr(numeric_only=True))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "c06139a9",
      "metadata": {},
      "source": [
        "if \"RR\" in daily_summary.columns:\n",
        "    rain_threshold = 0.0\n",
        "    daily_summary[\"rainy_day\"] = daily_summary[\"RR\"] > rain_threshold\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    if sns is not None:\n",
        "        sns.boxplot(data=daily_summary, x=\"rainy_day\", y=\"delay_count\")\n",
        "    else:\n",
        "        groups = [\n",
        "            daily_summary.loc[daily_summary[\"rainy_day\"], \"delay_count\"].dropna(),\n",
        "            daily_summary.loc[~daily_summary[\"rainy_day\"], \"delay_count\"].dropna(),\n",
        "        ]\n",
        "        plt.boxplot(groups, labels=[\"rainy\", \"dry\"])\n",
        "        plt.ylabel(\"delay_count\")\n",
        "    plt.title(\"Delay Count: Rainy vs Dry Days\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"RR column not found; skipping rainy vs dry boxplot.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "411b21c7",
      "metadata": {},
      "source": [
        "## Time Series Analytics\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "6b14e869",
      "metadata": {},
      "source": [
        "ts = daily_summary.sort_values(\"date\").copy()\n",
        "ts[\"rolling_7d\"] = ts[\"delay_count\"].rolling(7, min_periods=1).mean()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(ts[\"date\"], ts[\"delay_count\"], label=\"daily\")\n",
        "plt.plot(ts[\"date\"], ts[\"rolling_7d\"], label=\"7-day mean\")\n",
        "plt.title(\"Daily Delay Count with 7-day Rolling Mean\")\n",
        "plt.xlabel(\"date\")\n",
        "plt.ylabel(\"delay_count\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "ts[\"weekday\"] = ts[\"date\"].dt.day_name()\n",
        "weekday_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "weekday_summary = (\n",
        "    ts.groupby(\"weekday\")[\"delay_count\"].mean().reindex(weekday_order)\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "weekday_summary.plot(kind=\"bar\")\n",
        "plt.title(\"Average Delay Count by Weekday\")\n",
        "plt.xlabel(\"weekday\")\n",
        "plt.ylabel(\"avg delay_count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "threshold = ts[\"delay_count\"].quantile(0.99)\n",
        "event_days = ts[ts[\"delay_count\"] >= threshold]\n",
        "weather_cols = [c for c in [\"RR\", \"TL\", \"P\", \"FF\", \"SO\", \"S0\", \"RF\"] if c in ts.columns]\n",
        "display_cols = [\"date\", \"delay_count\"] + weather_cols\n",
        "event_days[display_cols].sort_values(\"delay_count\", ascending=False).head(10)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "785782e3",
      "metadata": {},
      "source": [
        "## Duration Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "93132366",
      "metadata": {},
      "source": [
        "dur = cleaned.dropna(subset=[\"start_time\", \"end_time\"]).copy()\n",
        "dur[\"duration_min\"] = (dur[\"end_time\"] - dur[\"start_time\"]).dt.total_seconds() / 60.0\n",
        "dur = dur[dur[\"duration_min\"] >= 0]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.hist(dur[\"duration_min\"].dropna(), bins=30)\n",
        "plt.title(\"Distribution of Delay Duration (minutes)\")\n",
        "plt.xlabel(\"minutes\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "if \"FF\" in dur.columns:\n",
        "    dur[\"wind_bin\"] = np.where(dur[\"FF\"] > 10, \"wind>10\", \"wind<=10\")\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    if sns is not None:\n",
        "        sns.boxplot(data=dur, x=\"wind_bin\", y=\"duration_min\")\n",
        "    else:\n",
        "        groups = [\n",
        "            dur.loc[dur[\"wind_bin\"] == \"wind>10\", \"duration_min\"].dropna(),\n",
        "            dur.loc[dur[\"wind_bin\"] == \"wind<=10\", \"duration_min\"].dropna(),\n",
        "        ]\n",
        "        plt.boxplot(groups, labels=[\"wind>10\", \"wind<=10\"])\n",
        "        plt.ylabel(\"duration_min\")\n",
        "    plt.title(\"Duration by Wind Bin (FF > 10)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"FF column not found; skipping wind bin analysis.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "eaa61487",
      "metadata": {},
      "source": [
        "## Predictive Model (High Delay Day)\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "ce7a6861",
      "metadata": {},
      "source": [
        "try:\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.metrics import classification_report, roc_auc_score\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "    model_df = daily_summary.dropna(subset=[\"date\", \"delay_count\"]).sort_values(\"date\").copy()\n",
        "    model_df[\"weekday\"] = model_df[\"date\"].dt.weekday\n",
        "    model_df[\"month\"] = model_df[\"date\"].dt.month\n",
        "\n",
        "    target = model_df[\"delay_count\"] >= model_df[\"delay_count\"].quantile(0.90)\n",
        "    model_df[\"high_delay\"] = target.astype(int)\n",
        "\n",
        "    feature_candidates = [\"RR\", \"TL\", \"P\", \"FF\", \"SO\", \"S0\", \"RF\", \"weekday\", \"month\"]\n",
        "    feature_cols = [c for c in feature_candidates if c in model_df.columns]\n",
        "    if not feature_cols:\n",
        "        raise ValueError(\"No feature columns available for modeling.\")\n",
        "\n",
        "    X = model_df[feature_cols].copy()\n",
        "    X = X.fillna(X.median(numeric_only=True))\n",
        "    y = model_df[\"high_delay\"]\n",
        "\n",
        "    split_idx = int(len(model_df) * 0.8)\n",
        "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
        "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
        "\n",
        "    model = Pipeline([\n",
        "        (\"scale\", StandardScaler()),\n",
        "        (\"clf\", LogisticRegression(max_iter=1000))\n",
        "    ])\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    print(classification_report(y_test, y_pred, digits=3))\n",
        "    try:\n",
        "        print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
        "    except ValueError as exc:\n",
        "        print(\"ROC AUC not available:\", exc)\n",
        "\n",
        "except ImportError as exc:\n",
        "    print(\"scikit-learn is not available. Install it to run the model.\")\n",
        "    print(exc)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "0a585696",
      "metadata": {},
      "source": [
        "## Text Analytics (Keywords and Weather Context)\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "24eb3959",
      "metadata": {},
      "source": [
        "import json\n",
        "\n",
        "from pymongo import MongoClient\n",
        "\n",
        "MONGO_URI = os.getenv(\"MONGO_URI\", \"mongodb://mongodb:27017\")\n",
        "DB_NAME = os.getenv(\"MONGO_DB\", \"big_data_austria\")\n",
        "WL_COLLECTION = os.getenv(\"WL_HIST_COLLECTION\", \"wienerlinien_historical\")\n",
        "HW_COLLECTION = os.getenv(\"HW_COLLECTION\", \"Station_HW_25\")\n",
        "\n",
        "def _parse_data(value):\n",
        "    if isinstance(value, dict):\n",
        "        return value\n",
        "    if isinstance(value, str):\n",
        "        try:\n",
        "            return json.loads(value)\n",
        "        except json.JSONDecodeError:\n",
        "            return {}\n",
        "    return {}\n",
        "\n",
        "def _normalize_wl(df):\n",
        "    if \"data\" not in df.columns:\n",
        "        return df\n",
        "    data_series = df[\"data\"].apply(_parse_data)\n",
        "    data_df = pd.json_normalize(data_series, sep=\"_\")\n",
        "    return pd.concat([df.drop(columns=[\"data\"]), data_df], axis=1)\n",
        "\n",
        "def _filter_stoerunglang(df):\n",
        "    if \"PartitionKey\" in df.columns:\n",
        "        df = df[~df[\"PartitionKey\"].fillna(\"\").str.lower().eq(\"stoerunglang\")]\n",
        "    if \"category\" in df.columns:\n",
        "        df = df[~df[\"category\"].fillna(\"\").str.lower().eq(\"stoerunglang\")]\n",
        "    return df\n",
        "\n",
        "client = MongoClient(MONGO_URI)\n",
        "db = client[DB_NAME]\n",
        "\n",
        "wl_docs = list(db[WL_COLLECTION].find({}, {\"_id\": 0}))\n",
        "hw_docs = list(db[HW_COLLECTION].find({}, {\"_id\": 0}))\n",
        "\n",
        "wl_df = pd.DataFrame(wl_docs)\n",
        "hw_df = pd.DataFrame(hw_docs)\n",
        "\n",
        "wl_df = _normalize_wl(wl_df)\n",
        "wl_df = _filter_stoerunglang(wl_df)\n",
        "\n",
        "if \"time_start\" not in wl_df.columns:\n",
        "    raise ValueError(\"Expected 'time_start' in Wiener Linien data for text analytics.\")\n",
        "\n",
        "wl_df[\"timestamp\"] = pd.to_datetime(wl_df[\"time_start\"], errors=\"coerce\", utc=True)\n",
        "wl_df = wl_df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n",
        "\n",
        "if \"timestamp\" not in hw_df.columns:\n",
        "    raise ValueError(\"HW 25 data has no 'timestamp' column.\")\n",
        "\n",
        "hw_df[\"timestamp\"] = pd.to_datetime(hw_df[\"timestamp\"], errors=\"coerce\", utc=True)\n",
        "hw_df = hw_df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n",
        "\n",
        "wl_weather = pd.merge_asof(\n",
        "    wl_df,\n",
        "    hw_df,\n",
        "    on=\"timestamp\",\n",
        "    direction=\"backward\",\n",
        ")\n",
        "\n",
        "text_col = None\n",
        "for candidate in [\"description\", \"data_description\", \"title\", \"data_title\"]:\n",
        "    if candidate in wl_weather.columns:\n",
        "        text_col = candidate\n",
        "        break\n",
        "\n",
        "if text_col is None:\n",
        "    raise ValueError(\"No text column found for keyword analysis.\")\n",
        "\n",
        "wl_weather[\"text\"] = wl_weather[text_col].fillna(\"\").str.lower()\n",
        "\n",
        "keywords = [\n",
        "    \"bauarbeiten\",\n",
        "    \"unfall\",\n",
        "    \"stoerung\",\n",
        "    \"signal\",\n",
        "    \"weiche\",\n",
        "    \"fahrzeug\",\n",
        "    \"polizei\",\n",
        "    \"defekt\",\n",
        "]\n",
        "\n",
        "for kw in keywords:\n",
        "    wl_weather[f\"kw_{kw}\"] = wl_weather[\"text\"].str.contains(kw, regex=False)\n",
        "\n",
        "if \"RR\" in wl_weather.columns:\n",
        "    wl_weather[\"rainy\"] = wl_weather[\"RR\"] > 0\n",
        "    keyword_cols = [f\"kw_{kw}\" for kw in keywords]\n",
        "    keyword_rates = (\n",
        "        wl_weather.groupby(\"rainy\")[keyword_cols].mean().T\n",
        "        .rename(columns={True: \"rainy\", False: \"dry\"})\n",
        "    )\n",
        "    keyword_rates.sort_values(by=\"rainy\", ascending=False).head(10)\n",
        "else:\n",
        "    print(\"RR column not available for rainy vs dry comparison.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}